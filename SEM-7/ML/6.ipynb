{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Yogesh Bhandare** *06*\n",
        "\n",
        "Assignment 6 Reinforcement Learning"
      ],
      "metadata": {
        "id": "yrEGq4sg4_pi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "1ShCO1D74_pq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. define the environment (maze)\n",
        "# r: regular space, g: goal (reward +10), s: start, w: wall (reward -10)\n",
        "# map the environment to a 4x4 grid of rewards for the agent\n",
        "environment_rewards = np.array([\n",
        "    [ 0,  0,  0,  0],\n",
        "    [ 0, -10,  0,  0],\n",
        "    [ 0,  0,  0,  0],\n",
        "    [ 0,  0,  0, 10] # goal state is 3,3 (reward 10)\n",
        "])"
      ],
      "metadata": {
        "id": "LNpUs37a5PWd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. initialize q-table\n",
        "# q-table has dimensions: (number of states) x (number of actions)\n",
        "# states: 16 (4x4 grid cells), actions: 4 (up, down, left, right)\n",
        "q_table = np.zeros((16, 4))\n",
        "\n",
        "# map actions to indices: 0: up, 1: down, 2: left, 3: right\n",
        "actions = {0: 'up', 1: 'down', 2: 'left', 3: 'right'}"
      ],
      "metadata": {
        "id": "M3h55tW55P1X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. define hyperparameters\n",
        "gamma = 0.9      # discount factor (importance of future rewards)\n",
        "alpha = 0.1      # learning rate (how much new info overrides old)\n",
        "epsilon = 0.1    # exploration rate (chance to take a random action)\n",
        "num_episodes = 1000"
      ],
      "metadata": {
        "id": "h2Du3Z4G5RMG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. helper functions for navigation and state\n",
        "def state_to_row_col(state):\n",
        "    # converts a 1d state index (0-15) to a 2d (row, col)\n",
        "    return state // 4, state % 4\n",
        "\n",
        "def get_next_state(row, col, action):\n",
        "    # determines the new (row, col) after an action\n",
        "    new_row, new_col = row, col\n",
        "    if action == 0: new_row = max(0, row - 1)  # up\n",
        "    elif action == 1: new_row = min(3, row + 1)  # down\n",
        "    elif action == 2: new_col = max(0, col - 1)  # left\n",
        "    elif action == 3: new_col = min(3, col + 1)  # right\n",
        "\n",
        "    # check for wall (the reward array handles the wall penalty)\n",
        "    if environment_rewards[new_row, new_col] == -10:\n",
        "        return row * 4 + col # stay in the same state if it's a wall\n",
        "\n",
        "    return new_row * 4 + new_col # new state index\n",
        "\n",
        "def get_reward(row, col):\n",
        "    # returns reward from the environment array\n",
        "    return environment_rewards[row, col]"
      ],
      "metadata": {
        "id": "hBQ0fR335RxP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 5. q-learning training loop\n",
        "for episode in range(num_episodes):\n",
        "    current_state = 0  # start at state 0 (row 0, col 0)\n",
        "\n",
        "    # continue until the goal state (15) is reached\n",
        "    while current_state != 15:\n",
        "        # epsilon-greedy strategy: explore (random action) or exploit (best action)\n",
        "        if np.random.uniform(0, 1) < epsilon:\n",
        "            action = np.random.randint(4)  # explore: choose a random action\n",
        "        else:\n",
        "            # exploit: choose the action with the max q-value for the current state\n",
        "            action = np.argmax(q_table[current_state, :])\n",
        "\n",
        "        # calculate next state and reward\n",
        "        row, col = state_to_row_col(current_state)\n",
        "        next_state = get_next_state(row, col, action)\n",
        "        reward = get_reward(*state_to_row_col(next_state))\n",
        "\n",
        "        # q-learning formula update: q(s,a) = q(s,a) + alpha * [reward + gamma * max(q(s',a')) - q(s,a)]\n",
        "        old_q = q_table[current_state, action]\n",
        "        max_future_q = np.max(q_table[next_state, :])\n",
        "\n",
        "        new_q = old_q + alpha * (reward + gamma * max_future_q - old_q)\n",
        "        q_table[current_state, action] = new_q\n",
        "\n",
        "        current_state = next_state"
      ],
      "metadata": {
        "id": "XjrQL-pL5SHr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 6. determine and print the final path\n",
        "print(\"q-table training complete.\")\n",
        "print(\"--- optimal path ---\")\n",
        "current_state = 0\n",
        "path = [current_state]\n",
        "while current_state != 15:\n",
        "    # exploit the learned knowledge (choose the best action)\n",
        "    best_action = np.argmax(q_table[current_state, :])\n",
        "    row, col = state_to_row_col(current_state)\n",
        "    current_state = get_next_state(row, col, best_action)\n",
        "    path.append(current_state)\n",
        "\n",
        "# map state indices back to (row, col) for visualization\n",
        "path_coordinates = [state_to_row_col(s) for s in path]\n",
        "print(f\"path (states 0-15): {path}\")\n",
        "print(f\"path (row, col): {path_coordinates}\")\n",
        "\n",
        "# visualize the maze path\n",
        "maze_map = np.full((4, 4), 'R', dtype=str)\n",
        "maze_map[0, 0] = 'S'  # start\n",
        "maze_map[1, 1] = 'W'  # wall\n",
        "maze_map[3, 3] = 'G'  # goal\n",
        "for r, c in path_coordinates:\n",
        "    if maze_map[r, c] == 'R':\n",
        "        maze_map[r, c] = '*' # mark path\n",
        "    elif maze_map[r, c] == 'W':\n",
        "        pass # don't overwrite wall\n",
        "print(\"\\n--- maze with optimal path (*) ---\")\n",
        "print(maze_map)"
      ],
      "metadata": {
        "id": "nXYOMaxw5Sgx"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}