{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e82e363b-c44a-463e-89c2-9b88269050ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Crawling: https://en.wikipedia.org/wiki/Data_science\n",
      "\n",
      " Total pages collected: 11\n",
      " Pages found:\n",
      "1. https://en.wikipedia.org/wiki/Data_science\n",
      "2. https://en.wikipedia.org/wiki/Business\n",
      "3. https://en.wikipedia.org/wiki/Supervised_learning\n",
      "4. https://en.wikipedia.org/wiki/Data_cooperative\n",
      "5. https://en.wikipedia.org/wiki/Open_data\n",
      "6. https://en.wikipedia.org/wiki/Computational_science\n",
      "7. https://en.wikipedia.org/wiki/Data_compression\n",
      "8. https://en.wikipedia.org/wiki/Data_philanthropy\n",
      "9. https://en.wikipedia.org/wiki/Data_visualization\n",
      "10. https://en.wikipedia.org/wiki/Algorithm\n",
      "11. https://en.wikipedia.org/wiki/Joanna_Bryson\n",
      "\n",
      " PageRank Scores:\n",
      "https://en.wikipedia.org/wiki/Business : 0.09156\n",
      "https://en.wikipedia.org/wiki/Supervised_learning : 0.09156\n",
      "https://en.wikipedia.org/wiki/Data_cooperative : 0.09156\n",
      "https://en.wikipedia.org/wiki/Open_data : 0.09156\n",
      "https://en.wikipedia.org/wiki/Computational_science : 0.09156\n",
      "https://en.wikipedia.org/wiki/Data_compression : 0.09156\n",
      "https://en.wikipedia.org/wiki/Data_philanthropy : 0.09156\n",
      "https://en.wikipedia.org/wiki/Data_visualization : 0.09156\n",
      "https://en.wikipedia.org/wiki/Algorithm : 0.09156\n",
      "https://en.wikipedia.org/wiki/Joanna_Bryson : 0.09156\n",
      "https://en.wikipedia.org/wiki/Data_science : 0.08439\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup, MarkupResemblesLocatorWarning\n",
    "import networkx as nx\n",
    "import warnings\n",
    "\n",
    "# Suppress warnings from BeautifulSoup\n",
    "warnings.filterwarnings(\"ignore\", category=MarkupResemblesLocatorWarning)\n",
    "\n",
    "CHROME_USER_AGENT = (\n",
    "    \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) \"\n",
    "    \"AppleWebKit/537.36 (KHTML, like Gecko) \"\n",
    "    \"Chrome/141.0.0.0 Safari/537.36\"\n",
    ")\n",
    "\n",
    "start_url = \"https://en.wikipedia.org/wiki/Data_science\"\n",
    "\n",
    "# Step 2: Function to extract internal Wikipedia links\n",
    "def get_links(url):\n",
    "    try:\n",
    "        headers = {\"User-Agent\": CHROME_USER_AGENT}\n",
    "        response = requests.get(url, headers=headers, timeout=10)\n",
    "\n",
    "        # Validate the response content\n",
    "        if not response.ok or not response.text.strip().startswith(\"<\"):\n",
    "            print(f\" Skipping invalid response from {url}\")\n",
    "            return []\n",
    "\n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "        links = set()\n",
    "\n",
    "        for a_tag in soup.find_all(\"a\", href=True):\n",
    "            href = a_tag[\"href\"]\n",
    "            if href.startswith(\"/wiki/\") and not any(prefix in href for prefix in [\":\", \"#\", \"Main_Page\"]):\n",
    "                links.add(\"https://en.wikipedia.org\" + href)\n",
    "\n",
    "        return list(links)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\" Error fetching {url}: {e}\")\n",
    "        return []\n",
    "\n",
    "# Step 3: Crawl a few pages and build directed graph\n",
    "pages_to_crawl = [start_url]\n",
    "max_pages = 5\n",
    "graph = nx.DiGraph()\n",
    "\n",
    "while pages_to_crawl and len(graph.nodes) < max_pages:\n",
    "    url = pages_to_crawl.pop(0)\n",
    "    if url not in graph.nodes:\n",
    "        print(f\" Crawling: {url}\")\n",
    "        links = get_links(url)\n",
    "        for link in links[:10]:\n",
    "            graph.add_edge(url, link)\n",
    "        pages_to_crawl.extend(links[:2])\n",
    "\n",
    "print(\"\\n Total pages collected:\", len(graph.nodes))\n",
    "print(\" Pages found:\")\n",
    "for i, page in enumerate(graph.nodes, 1):\n",
    "    print(f\"{i}. {page}\")\n",
    "\n",
    "# Step 4: Compute PageRank\n",
    "pagerank_scores = nx.pagerank(graph)\n",
    "\n",
    "# Step 5: Display results\n",
    "print(\"\\n PageRank Scores:\")\n",
    "for page, rank in sorted(pagerank_scores.items(), key=lambda x: x[1], reverse=True):\n",
    "    print(f\"{page} : {rank:.5f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "378d7a5e-2a10-4b1a-9bf9-c4cfb50510a9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
