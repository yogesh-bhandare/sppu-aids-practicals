{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "7) Text Analytics\n",
        "1. Extract Sample document and apply following document preprocessing methods:\n",
        "Tokenization, POS Tagging, stop words removal, Stemming and Lemmatization.\n",
        "2. Create representation of documents by calculating Term Frequency and Inverse\n",
        "DocumentFrequency."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2eERxLvUaDz6",
        "outputId": "84f1daa9-3669-485b-dd3d-f8a8a80ca253"
      },
      "outputs": [],
      "source": [
        "import nltk   # Natural Language Tool Kit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to\n",
            "[nltk_data]     C:\\Users\\yogesh\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['Hello', 'everyone', '!', ',', 'Welcome', 'to', 'my', 'blog', 'post', 'on', 'Medium', '.', 'We', 'are', 'studying', 'Natural', 'Language', 'Processing', '.']\n",
            "\n",
            "['Hello everyone!, Welcome to my blog post on Medium.', 'We are studying Natural Language Processing.']\n"
          ]
        }
      ],
      "source": [
        "# Tokenization\n",
        "from nltk import word_tokenize, sent_tokenize\n",
        "sent = \"Hello everyone!, Welcome to my blog post on Medium. We are studying Natural Language Processing.\"\n",
        "print(word_tokenize(sent))\n",
        "print()\n",
        "print(sent_tokenize(sent))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     C:\\Users\\yogesh\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# stopwords\n",
        "nltk.download('stopwords')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "This is the unclean version: ['Hello', 'everyone', '!', ',', 'Welcome', 'to', 'my', 'blog', 'post', 'on', 'Medium', '.', 'We', 'are', 'studying', 'Natural', 'Language', 'Processing', '.']\n",
            "This is the cleaned version: ['Hello', 'everyone', '!', ',', 'Welcome', 'blog', 'post', 'Medium', '.', 'We', 'studying', 'Natural', 'Language', 'Processing', '.']\n"
          ]
        }
      ],
      "source": [
        "from nltk.corpus import stopwords        # the corpus module is an extremely useful one. \n",
        "                                         \n",
        "stop_words = stopwords.words('english')  # this is the full list of all stop-words stored in nltk\n",
        "\n",
        "token = word_tokenize(sent)\n",
        "cleaned_token = []\n",
        "for word in token:\n",
        "    if word not in stop_words:\n",
        "        cleaned_token.append(word)\n",
        "print(\"This is the unclean version:\", token)\n",
        "print(\"This is the cleaned version:\", cleaned_token)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['this', 'is', 'a', 'demo', 'text', 'for', 'nlp', 'use', 'nltk', '.', 'full', 'form', 'of', 'nltk', 'is', 'natur', 'languag', 'toolkit']\n"
          ]
        }
      ],
      "source": [
        "from nltk.stem import SnowballStemmer\n",
        "snowball_stemmer = SnowballStemmer('english')\n",
        "text=\"This is a Demo Text for NLP using NLTK. Full form of NLTK is Natural Language Toolkit\"\n",
        "word_tokens = nltk.word_tokenize(text)\n",
        "stemmed_word = [snowball_stemmer.stem(word) for word in word_tokens]\n",
        "print (stemmed_word)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to\n",
            "[nltk_data]     C:\\Users\\yogesh\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "nltk.download('wordnet')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['the', 'dog', 'are', 'barking', 'outside', '.', 'Are', 'the', 'cat', 'in', 'the', 'garden', '?']\n"
          ]
        }
      ],
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "#is based on The Porter Stemming Algorithm\n",
        "stopword = stopwords.words('english')\n",
        "wordnet_lemmatizer = WordNetLemmatizer()\n",
        "text = \"the dogs are barking outside. Are the cats in the garden?\"\n",
        "word_tokens = nltk.word_tokenize(text)\n",
        "lemmatized_word = [wordnet_lemmatizer.lemmatize(word) for word in word_tokens]\n",
        "print (lemmatized_word)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     C:\\Users\\yogesh\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "nltk.download('averaged_perceptron_tagger')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[('the', 'DT'), ('dogs', 'NNS'), ('are', 'VBP'), ('barking', 'VBG'), ('outside', 'IN'), ('.', '.')]\n"
          ]
        }
      ],
      "source": [
        "text = \"the dogs are barking outside.\"\n",
        "word = nltk.word_tokenize(text)\n",
        "pos_tag = nltk.pos_tag(word)\n",
        "print (pos_tag)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 2. Create representation of document by calculating Term Frequency and Inverse Document Frequency.\n",
        "# import required module\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "d0 = 'New York Times'\n",
        "d1 = 'New York Post'\n",
        "d2 = 'Los Angles Times'\n",
        "\n",
        "series = [d0, d1, d2]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "# create object\n",
        "tfidf = TfidfVectorizer()\n",
        "\n",
        "# get tf-df values\n",
        "result = tfidf.fit_transform(series)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Word indexes:\n",
            "{'new': 2, 'york': 5, 'times': 4, 'post': 3, 'los': 1, 'angles': 0}\n",
            "\n",
            "tf-idf value:\n",
            "<Compressed Sparse Row sparse matrix of dtype 'float64'\n",
            "\twith 9 stored elements and shape (3, 6)>\n",
            "  Coords\tValues\n",
            "  (0, 2)\t0.5773502691896257\n",
            "  (0, 5)\t0.5773502691896257\n",
            "  (0, 4)\t0.5773502691896257\n",
            "  (1, 2)\t0.5178561161676974\n",
            "  (1, 5)\t0.5178561161676974\n",
            "  (1, 3)\t0.680918560398684\n",
            "  (2, 4)\t0.4736296010332684\n",
            "  (2, 1)\t0.6227660078332259\n",
            "  (2, 0)\t0.6227660078332259\n",
            "\n",
            "tf-idf values in matrix form:\n",
            "[[0.         0.         0.57735027 0.         0.57735027 0.57735027]\n",
            " [0.         0.         0.51785612 0.68091856 0.         0.51785612]\n",
            " [0.62276601 0.62276601 0.         0.         0.4736296  0.        ]]\n"
          ]
        }
      ],
      "source": [
        "# get indexing\n",
        "print('\\nWord indexes:')\n",
        "print(tfidf.vocabulary_)\n",
        "\n",
        "# display tf-idf values\n",
        "print('\\ntf-idf value:')\n",
        "print(result)\n",
        "\n",
        "# in matrix form\n",
        "print('\\ntf-idf values in matrix form:')\n",
        "print(result.toarray())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "Welcome To Colab",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
